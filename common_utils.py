from dotenv import load_dotenv
from fastapi import HTTPException
import sys
sys.path.append('.')
sys.path.append('..')
import pinecone_utils
import gpt_utils
import api_utils
import json
import concurrent.futures
import traceback
import os
import re
import s3_bucket_utils
import common_utils
import time
from collections import defaultdict, deque


load_dotenv()


API_KEY=os.getenv("API_KEY")
API_SECRET=os.getenv("API_SECRET")


def validate_request(api_secret,api_key):
    return (api_secret == API_SECRET and api_key == API_KEY)


def validate_prompt_labels(prompt_labels):
    if not isinstance(prompt_labels, list):
        raise HTTPException(status_code=400, detail="Prompt labels must be provided as a list.")

    if len(prompt_labels) > 5:
        raise HTTPException(status_code=400, detail="Too many prompt labels provided. Maximum allowed is 5.")

    valid_labels = ['P1', 'P2', 'P3', 'P4', 'P5']
    for label in prompt_labels:
        if label not in valid_labels:
            raise HTTPException(status_code=400, detail=f"Invalid prompt label '{label}'. Prompt labels must be from P1 to P5.")

    return True, "Prompt labels are valid."


def filter_dependencies(prompt_labels, dependencies):
    # Filter the dependencies dictionary to only include prompt labels in prompt_labels
    filtered_dependencies = {k: v for k, v in dependencies.items() if k in prompt_labels}

    # Update the remaining dependencies to only include those in prompt_labels
    for key in filtered_dependencies:
        filtered_dependencies[key] = [dep for dep in filtered_dependencies[key] if dep in prompt_labels]
    
    return filtered_dependencies


def process_prompts(selected_prompts):
    for prompt_label, prompt_data in selected_prompts.items():
        for prompt in prompt_data:
            print(prompt["prompt_label"])
    return selected_prompts


def analyze_text(summary_text, prompt_label, prompt_info, wisdom, filters, pdf_mappings, model, tokens_counter=None, dependency_comments=None):
    
    generated_prompt = prompt_info["generated_prompt"]
    prompt_chunks_count = prompt_info["chunks"]
    prompt_corpus = prompt_info["prompt_corpus"]
        
    if(prompt_corpus):
        filter = filters[prompt_corpus]
    else:
        print("Setting filter as none for P_Custom")
        # filter = {"Organization": {"$eq": organization_id}}
        filter = None
    
    chunk_match_text = f"{summary_text}\n{generated_prompt}"
    
    relevant_chunks = pinecone_utils.extract_unique_chunks(chunk_match_text, top_k=prompt_chunks_count, multiplier=2, filter=filter)    
    
    context = relevant_chunks['all_context']
    
    wisdom1 = None
    wisdom2 = None
    if prompt_label != "P_Custom":
        wisdom1 = wisdom[prompt_label][0]
        wisdom2 = wisdom[prompt_label][1]
    
    sources_info = {}
    for i in range(prompt_chunks_count):
        try:
            pdf_name = relevant_chunks[f'meta_{i+1}']
            pdf_url = ""
            pdf_info = {}

            if pdf_name in pdf_mappings: 
                pdf_info = pdf_mappings[pdf_name]
                pdf_url = pdf_info["link"]

            sources_info[f'chunk_id_{i+1}'] = {"name": pdf_name, "url": pdf_url}
        except Exception as e:
            pass
            #traceback.print_exc()

    generated_comments, context = gpt_utils.generate_analyze_comments(generated_prompt, context, model, prompt_label, tokens_counter, wisdom1, wisdom2, comments=dependency_comments)
        
    analyze_comments = []
    try:
        generated_comments_json = json.loads(generated_comments)
        sources = list(sources_info.values())
    
        if isinstance(generated_comments_json['Comments'], list):
            for comment in generated_comments_json['Comments']:
                if isinstance(comment, dict):
                    for key, value in comment.items():
                        analyze_comments.append({"comment": f"{key}: {value}", "sources": sources})
                else:
                    analyze_comments.append({"comment": comment, "sources": sources})
        elif isinstance(generated_comments_json['Comments'], str):
            analyze_comments.append({"comment": generated_comments_json['Comments'], "sources": sources})
        else:
            raise Exception(f"Invalid value for generated_comments_json['Comments'],{generated_comments_json}")
    except Exception as e:
        print(f"Analyze Comment Exception: {str(e)}")
        traceback.print_exc() 
        analyze_comments={"comment":"Invalid comment generated by GPT","sources":[]}

    return prompt_corpus, analyze_comments, prompt_label, context


def analyze_text_test(summary_text, prompt_label, prompt_info, wisdom, filters, pdf_mappings, model, tokens_counter=None):
    print(f"Analyzing text for {prompt_label}...")
    start_time = time.time()
    time.sleep(15)
    end_time = time.time()
    print(f"Analyzed text for {prompt_label} in {end_time - start_time} seconds.")
    analyze_comments = "comments"
    return "corpus", analyze_comments, prompt_label, "context"


def generate_customised_prompt(selected_model, nature, prompt_label, summary_text, prompt_data, use_example, tokens_counter=None):
    prompt_for_customization, prompt_string, chunks, prompt_corpus, prompt_examples = prompt_data
    generated_prompt = gpt_utils.generate_prompt(prompt_for_customization, selected_model, prompt_string, nature, summary_text, use_example, prompt_examples, prompt_label, tokens_counter)
    return prompt_label, generated_prompt, chunks, prompt_corpus


def generate_prompts(selected_model, nature, summary_text, use_example, selected_prompts, tokens_counter=None):
    generated_prompts={}
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(generate_customised_prompt, selected_model, nature, prompt_label, summary_text, prompt_data, use_example, tokens_counter) for prompt_label, prompt_data in selected_prompts.items()]
        for future in concurrent.futures.as_completed(futures):
            prompt_label, generated_prompt, chunks, prompt_corpus = future.result()
            generated_prompts[prompt_label] = {"generated_prompt":generated_prompt,"chunks":chunks,"prompt_corpus":prompt_corpus}
            print(f"Generated Analyze prompt for {prompt_label}...")
    
    return generated_prompts  


def generate_evaluator_customised_prompt(selected_model, nature, prompt_label, tor_summary_text, prompt_data, tokens_counter=None):
    prompt_for_customization = prompt_data["customization_prompt"]
    base_prompt = prompt_data["base_prompt"]
    chunks = prompt_data["chunks"]
    if tor_summary_text == None:
        generated_prompt = base_prompt
    else:
        generated_prompt = gpt_utils.generate_evaluator_prompt(prompt_for_customization, selected_model, base_prompt, nature, tor_summary_text, prompt_label, tokens_counter)
    
    return prompt_label, generated_prompt, chunks


def generate_evaluator_prompts(selected_model, nature, tor_summary_text, evaluator_prompts, tokens_counter=None):
    generated_prompts={}
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(generate_evaluator_customised_prompt, selected_model, nature, prompt_label, tor_summary_text, prompt_data, tokens_counter) for prompt_label, prompt_data in evaluator_prompts.items()]
        for future in concurrent.futures.as_completed(futures):
            prompt_label, generated_prompt, chunks = future.result()
            generated_prompts[prompt_label] = generated_prompt
            print(f"Generated Evaluator prompt for {prompt_label}")
    return generated_prompts  


def generate_analyze_and_evaluator_prompts(
    prompt_model, nature_of_document, proposal_summary_text, use_example, analyzer_prompts, tor_summary_text, prompts_configurations, tokens_counter=None
    ):
    """Generate analyze and evaluator prompts concurrently"""
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        analyze_future = executor.submit(
            generate_prompts,
            prompt_model, nature_of_document, proposal_summary_text, use_example, analyzer_prompts, tokens_counter
        )
        evaluator_future = executor.submit(
            generate_evaluator_prompts,
            prompt_model, nature_of_document, tor_summary_text, prompts_configurations, tokens_counter
        )
        
        generated_analyze_prompts = analyze_future.result()
        generated_evaluator_prompts = evaluator_future.result()

    return generated_analyze_prompts, generated_evaluator_prompts


def generate_analyze_comments(summary_text, filters, pdf_mappings, model, generated_prompts, wisdom, prompts_section_title, nature, st=None, tokens_counter=None):
    generated_analyze_comments={}
    analyze_context_used={}
    results = []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(analyze_text, summary_text, prompt_label, prompt_info, wisdom, filters, pdf_mappings, model, tokens_counter) for prompt_label, prompt_info in generated_prompts.items()]
        for future in concurrent.futures.as_completed(futures):
            prompt_corpus, analyze_comments, prompt_label, context = future.result()
            section_title = prompts_section_title[prompt_label]
            prompt_analyze_comments={"section_title": section_title,"prompt_corpus":prompt_corpus,"analyze_comments":analyze_comments}
            generated_analyze_comments[prompt_label]=prompt_analyze_comments
            analyze_context_used[prompt_label]=context
            prompt_analyze_comments[f"Context used to generate {prompt_label} comments."] = context
            results.append({f"{prompt_label}  {prompt_corpus}:": prompt_analyze_comments})
            print("Generated Analyze comments for ", prompt_label)
      
    summary_prompt = api_utils.get_analyzer_comments_summary_prompts(nature)
    concatenated_comments = combine_comment(generated_analyze_comments)
    P0_summary = gpt_utils.get_P0_summary(concatenated_comments, model, nature, tokens_counter)
    generated_analyze_comments["P0"] = P0_summary
          
    if st:
        try:
            with st.expander(f"P0 Summary"):
                st.write({"P0": P0_summary})
            print("Successfully displayed P0 Summary")
        except Exception as e:
            print(f"Error displaying P0 Summary: {str(e)}")

        print(f"Results: {results}")

        for result in results:
            for key, value in sorted(result.items()):
                try:
                    with st.expander(key):
                        st.write(value)
                    print(f"Successfully displayed {key}")
                except Exception as e:
                    print(f"Error displaying {key}: {str(e)}")

        try:
            with st.expander(f"P0 Prompt"):
                st.write(summary_prompt)
            print("Successfully displayed P0 Prompt")
        except Exception as e:
            print(f"Error displaying P0 Prompt: {str(e)}")

        try:
            showcase = pinecone_utils.extract_unique_showcase_chunks(P0_summary, 10)
            showcase = common_utils.convert_showcase_to_s3_urls(showcase)
            with st.expander("Showcase"):
                st.json(showcase)
            print("Successfully displayed Showcase")
        except Exception as e:
            print(f"Error processing or displaying Showcase: {str(e)}")
                    
    return generated_analyze_comments, analyze_context_used


def generate_analyze_comments_test(summary_text, filters, pdf_mappings, model, generated_prompts, wisdom, prompts_section_title, nature, st=None, tokens_counter=None, dependency_graph=None):
    generated_analyze_comments = {}
    analyze_context_used = {}
    results = []

    if dependency_graph is None:
        dependency_graph = {key: [] for key in generated_prompts.keys()}
    
    def execute_task(prompt_label):
        print(f"Starting execution of {prompt_label}")
        
        dependency_comments = {}
        comments = ""
        if prompt_label in dependency_graph:
            for i, dep in enumerate(dependency_graph[prompt_label]):
                if dep in generated_analyze_comments:
                    for item in generated_analyze_comments[dep]["analyze_comments"]:
                        comments += item["comment"] + "\n"
                    dependency_comments[f"Context {i+1}"] = comments
        
        prompt_info = generated_prompts[prompt_label]
        prompt_corpus, analyze_comments, _, context = analyze_text(summary_text, prompt_label, prompt_info, wisdom, filters, pdf_mappings, model, tokens_counter, dependency_comments)
        section_title = prompts_section_title[prompt_label]
        prompt_analyze_comments = {"section_title": section_title, "prompt_corpus": prompt_corpus, "analyze_comments": analyze_comments}
        generated_analyze_comments[prompt_label] = prompt_analyze_comments
        analyze_context_used[prompt_label] = context
        prompt_analyze_comments[f"Context used to generate {prompt_label} comments."] = context
        results.append({f"{prompt_label}  {prompt_corpus}:": prompt_analyze_comments})
        
        return prompt_label

    sorted_tasks = [key for key in generated_prompts.keys()]
    completed_tasks = set()

    overall_start_time = time.time()
    print("Starting execution of all tasks")

    with concurrent.futures.ThreadPoolExecutor() as executor:
        while sorted_tasks:
            ready_tasks = [task for task in sorted_tasks if all(dep in completed_tasks for dep in dependency_graph.get(task, []))]
            print(f"Ready tasks: {ready_tasks}")

            futures = {executor.submit(execute_task, task): task for task in ready_tasks}

            for future in concurrent.futures.as_completed(futures):
                task = futures[future]
                completed_task = future.result()
                completed_tasks.add(completed_task)
                sorted_tasks.remove(completed_task)
                print(f"Completed tasks: {completed_tasks}")

    overall_end_time = time.time()
    print(f"All tasks completed. Total time taken: {overall_end_time - overall_start_time:.2f} seconds")

    print("Generating P0 Summary!")
    summary_prompt = api_utils.get_analyzer_comments_summary_prompts(nature)
    concatenated_comments = combine_comment(generated_analyze_comments)
    P0_summary = gpt_utils.get_P0_summary(concatenated_comments, model, nature, tokens_counter)
    generated_analyze_comments["P0"] = P0_summary
          
    if st: 
        with st.expander(f"P0 Summary"):
            st.write({"P0": P0_summary})
        for result in results:
            for key, value in sorted(result.items()):
                with st.expander(key):
                    st.write(value)
        with st.expander(f"P0 Prompt"):
            st.write(summary_prompt)
        showcase = pinecone_utils.extract_unique_showcase_chunks(P0_summary, 10)
        showcase = common_utils.convert_showcase_to_s3_urls(showcase)
        with st.expander("Showcase"):
            st.json(showcase)
                    
    return generated_analyze_comments, analyze_context_used


def generate_analyze_comments_evaluator(summary_text, filters, pdf_mappings, model, generated_prompts, wisdom, nature, st=None, tokens_counter=None, dependency_graph=None):
    generated_analyze_comments = {}
    analyze_context_used = {}
    results = []

    if dependency_graph is None:
        dependency_graph = {key: [] for key in generated_prompts.keys()}
    
    def execute_task(prompt_label):
        start_time = time.time()
        print(f"Starting execution of {prompt_label}")
        
        dependency_comments = {}
        comments = ""
        if prompt_label in dependency_graph:
            for i, dep in enumerate(dependency_graph[prompt_label]):
                if dep in generated_analyze_comments:
                    for item in generated_analyze_comments[dep]["analyze_comments"]:
                        comments += item["comment"] + "\n"
                    dependency_comments[f"Context {i+1}"] = comments
        
        prompt_info = generated_prompts[prompt_label]
        prompt_corpus, analyze_comments, _, context = analyze_text(summary_text, prompt_label, prompt_info, wisdom, filters, pdf_mappings, model, tokens_counter, dependency_comments)
        prompt_analyze_comments = {"prompt_corpus": prompt_corpus, "analyze_comments": analyze_comments}
        generated_analyze_comments[prompt_label] = prompt_analyze_comments
        analyze_context_used[prompt_label] = context
        prompt_analyze_comments[f"Context used to generate {prompt_label} comments."] = context
        results.append({f"{prompt_label}  {prompt_corpus}:": prompt_analyze_comments})
        
        end_time = time.time()
        print(f"Completed execution of {prompt_label}. Time taken: {end_time - start_time:.2f} seconds")
        return prompt_label

    sorted_tasks = [key for key in generated_prompts.keys()]
    completed_tasks = set()

    overall_start_time = time.time()
    print("Starting execution of all tasks")

    with concurrent.futures.ThreadPoolExecutor() as executor:
        while sorted_tasks:
            ready_tasks = [task for task in sorted_tasks if all(dep in completed_tasks for dep in dependency_graph.get(task, []))]
            print(f"Ready tasks: {ready_tasks}")

            futures = {executor.submit(execute_task, task): task for task in ready_tasks}

            for future in concurrent.futures.as_completed(futures):
                task = futures[future]
                completed_task = future.result()
                completed_tasks.add(completed_task)
                sorted_tasks.remove(completed_task)
                print(f"Completed tasks: {completed_tasks}")

    overall_end_time = time.time()
    print(f"All tasks completed. Total time taken: {overall_end_time - overall_start_time:.2f} seconds")
            
    return generated_analyze_comments, analyze_context_used


def generate_custom_analyze_comments(summary_text, filters, pdf_mappings, model, generated_prompts, wisdom, nature, tokens_counter=None):
    generated_analyze_comments={}
    analyze_context_used={}
    results = []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(analyze_text, summary_text, prompt_label, prompt_info, wisdom, filters, pdf_mappings, model, tokens_counter) for prompt_label, prompt_info in generated_prompts.items()]
        for future in concurrent.futures.as_completed(futures):
            prompt_corpus, analyze_comments, prompt_label,context = future.result()
            print(f"Generated Analyze comments for {prompt_label}")
            prompt_analyze_comments={"prompt_corpus":prompt_corpus,"analyze_comments":analyze_comments}
            generated_analyze_comments[prompt_label]=prompt_analyze_comments
            analyze_context_used[prompt_label]=context
            prompt_analyze_comments[f"Context used to generate {prompt_label} comments."] = context
            results.append({f"{prompt_label}  {prompt_corpus}:": prompt_analyze_comments})
            
    return generated_analyze_comments, analyze_context_used


def check_and_add_user_role_prompt(prompt_string, user_role):
    desired_string = f"User Role: You are a {user_role}."

    if not prompt_string.lower().startswith(desired_string.lower()):
        prompt_string = f"{desired_string}\n{prompt_string}"

    return prompt_string


def replace_placeholder(string, placeholder, value):
    return re.sub(r'\{' + re.escape(placeholder) + r'\}', value, string)


def has_placeholder(string, placeholder):
    return re.search(r'\{' + re.escape(placeholder) + r'\}', string) is not None


def topological_sort(dependencies):
    in_degree = defaultdict(int)
    graph = defaultdict(list)

    # Initialize in_degree for all nodes
    for node in dependencies:
        if node not in in_degree:
            in_degree[node] = 0
        for dep in dependencies[node]:
            graph[dep].append(node)
            in_degree[node] += 1
            if dep not in in_degree:
                in_degree[dep] = 0

    # Queue for nodes with no incoming edges (in-degree 0)
    queue = deque([node for node in in_degree if in_degree[node] == 0])
    topo_order = []

    while queue:
        node = queue.popleft()
        topo_order.append(node)
        for neighbor in graph[node]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)

    # If all nodes are processed, return topo_order, otherwise return an empty list
    if len(topo_order) == len(in_degree):
        return topo_order
    else:
        return []


def extract_sources_from_analyze_comments(generated_analyze_comments):
    all_sources = []

    for key, value in generated_analyze_comments.items():
        analyze_comments = value.get('analyze_comments', [])

        for comment_data in analyze_comments:
            sources = comment_data.get('sources', [])
            for source in sources:
                all_sources.append(source)

    print(f"Found a total of {len(all_sources)} sources.")

    unique_entries = []
    seen_combinations = set()

    for entry in all_sources:
        name = entry["name"]
        url = entry["url"]
        combination = (name, url)

        if combination not in seen_combinations:
            unique_entries.append(entry)
            seen_combinations.add(combination)

    print(f"Found {len(unique_entries)} unique sources.")
    return unique_entries


def combine_comment(generated_analyze_comments):
    combined_comments = ""
    
    # Combine all comments
    for key, value in generated_analyze_comments.items():
        if "analyze_comments" in value:
            for item in value["analyze_comments"]:
                combined_comments += item["comment"] + "\n"
                
    return combined_comments.strip()


def merge_chunks(relevant_chunks_f1, relevant_chunks_f2, relevant_chunks_f3):
    merged = {}
    seen_values = set()
    counter = {
        'meta': 1,
        'url': 1,
        'context': 1,
        'publication_year': 1,
        'reference': 1,
        'country': 1,
        'author': 1,
        'chunk_id': 1,
        'context': 1,
        'metadata': 1
    }
    
    for d in (relevant_chunks_f1, relevant_chunks_f2, relevant_chunks_f3):
        for key, value in d.items():
            if value not in seen_values:
                base_key = key.split('_')[0]
                new_key = f"{base_key}_{counter[base_key]}"
                merged[new_key] = value
                seen_values.add(value)
                counter[base_key] += 1
    
    return merged


def merge_unique_chunks(prompt_label, which_chunks, relevant_chunks_f1, relevant_chunks_f2, relevant_chunks_f3):
    relevant_chunks = {}
    chunks_to_merge = []
    
    if f'{prompt_label}.F1' in which_chunks and len(relevant_chunks_f1) > 0:
        chunks_to_merge.append(relevant_chunks_f1)
    if f'{prompt_label}.F2' in which_chunks and len(relevant_chunks_f2) > 0:
        chunks_to_merge.append(relevant_chunks_f2)
    if f'{prompt_label}.F3' in which_chunks and len(relevant_chunks_f3) > 0:
        chunks_to_merge.append(relevant_chunks_f3)
    
    if len(chunks_to_merge) > 1:
        for chunks in chunks_to_merge:
            if 'all_context' in chunks:
                del chunks['all_context']
        relevant_chunks = merge_chunks(*chunks_to_merge)
    elif len(chunks_to_merge) == 1:
        relevant_chunks = chunks_to_merge[0]
    
    return relevant_chunks


def get_text(text):
    input_length_limit = int(os.getenv("summary_pdf_text_length"))
    return text[:input_length_limit]


def extract_base_customization_prompts(selected_prompts):
    extracted_dict = {}
    for key, values in selected_prompts.items():
        base_prompt = values[1]
        customization_prompt = values[0]
        extracted_dict[key] = {"Base Prompt": base_prompt, "Customization Prompt": customization_prompt}
    return extracted_dict


def get_unique_sources(sources_info):
    
    unique_dicts = set()

    for d in sources_info:
        frozen_d = frozenset(d.items())
        unique_dicts.add(frozen_d)

    result = [dict(fd) for fd in unique_dicts]
    return result


def break_into_paragraphs(text: str):
    paragraphs = text.split("\n\n")  
    
    paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip()]
    
    return paragraphs


def convert_showcase_to_s3_urls(showcase):
    for showcase_item in showcase:
        organisation = showcase_item["organisation"]
        showcase_title = showcase_item['showcase_title']
        organisation_logo_filename = s3_bucket_utils.convert_showcase_to_s3_img_filename(organisation)
        showcase_logo_filename = s3_bucket_utils.convert_showcase_to_s3_img_filename(showcase_title)
        showcase_item['organisation_logo'] = s3_bucket_utils.get_file_par_url(f"showcase/{organisation_logo_filename}")
        showcase_item['showcase_image'] = s3_bucket_utils.get_file_par_url(f"showcase/{showcase_logo_filename}")
    return showcase


def get_analyzer_tokens_counter():
    sub_keys = ['p_custom'] + [f'p{i}' for i in range(1, 6)]
    
    nested_dict_prompts1 = {key: 0 for key in sub_keys}
    nested_dict_comments1 = {key: 0 for key in sub_keys}
    nested_dict_prompts2 = {key: 0 for key in sub_keys}
    nested_dict_comments2 = {key: 0 for key in sub_keys}

    token_counters1 = {
        'analyzer_prompts': nested_dict_prompts1,
        'analyzer_comments': nested_dict_comments1,
        'proposal_summary': 0,
        'p0_summary': 0
    }

    token_counters2 = {
        'analyzer_prompts': nested_dict_prompts2,
        'analyzer_comments': nested_dict_comments2,
        'proposal_summary': 0,
        'p0_summary': 0,
        "total_tokens": 0
    }

    return (token_counters1, token_counters2)


def get_evaluator_tokens_counter():
    sub_keys = [f'p{i}' for i in range(1,6)]
    sub_keys2 = ['p_internal', 'p_external', 'p_delta']
    
    nested_dict_prompts1 = {key: 0 for key in sub_keys}
    nested_dict_comments1 = {key: 0 for key in sub_keys}
    nested_dict_prompts2 = {key: 0 for key in sub_keys}
    nested_dict_comments2 = {key: 0 for key in sub_keys}
    
    nested_dict_prompts3 = {key: 0 for key in sub_keys2}
    nested_dict_comments3 = {key: 0 for key in sub_keys2}
    nested_dict_prompts4 = {key: 0 for key in sub_keys2}
    nested_dict_comments4 = {key: 0 for key in sub_keys2}

    token_counters1 = {
        'analyzer_prompts': nested_dict_prompts1,
        'analyzer_comments': nested_dict_comments1,
        'evaluator_prompts': nested_dict_prompts3,
        'evaluator_comments': nested_dict_comments3,
        'proposal_summary': 0,
        'tor_summary': 0
    }

    token_counters2 = {
        'analyzer_prompts': nested_dict_prompts2,
        'analyzer_comments': nested_dict_comments2,
        'evaluator_prompts': nested_dict_prompts4,
        'evaluator_comments': nested_dict_comments4,
        'proposal_summary': 0,
        'tor_summary': 0,
        "total_tokens": 0
    }

    return (token_counters1, token_counters2)


def total_tokens_count(data):
    total_tokens = 0

    # Iterate over each key and value in the dictionary
    for key, value in data.items():
        if isinstance(value, dict):
            # If the value is a dictionary, recursively call this function
            total_tokens += total_tokens_count(value)
        elif isinstance(value, int):
            # If the value is an integer, add it to the total_tokens
            total_tokens += value
        else:
            raise ValueError("Unexpected data type in dictionary")

    return total_tokens


def update_parent_dependencies(parent_dict, child_dict):
    parent_to_children = {}
    for child in child_dict.keys():
        parent = child.split('.')[0]
        if parent not in parent_to_children:
            parent_to_children[parent] = []
        parent_to_children[parent].append(child)

    for parent in parent_dict:
        for child in parent_to_children.get(parent, []):
            for dep in child_dict[child]:
                if '.' in dep:
                    dep_parent = dep.split('.')[0]
                else:
                    dep_parent = dep
                if dep_parent != parent and dep_parent not in parent_dict[parent]:
                    parent_dict[parent].append(dep_parent)

    return parent_dict


    